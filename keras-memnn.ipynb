{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Activation, Dense, Lambda, Permute, Dropout, add, dot, concatenate\n",
    "from keras.layers import LSTM\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from functools import reduce\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from itertools import chain\n",
    "from process_data import load_entities, save_pickle, load_pickle, load_kv_pairs, lower_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    '''Return the tokens of a sentence including punctuation.\n",
    "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
    "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
    "    '''\n",
    "    return [x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]\n",
    "\n",
    "def load_task(fpath, is_babi, max_length=None):\n",
    "    with open (fpath, encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        data, story = [], []\n",
    "        for l in lines:\n",
    "            l = l.rstrip()\n",
    "            turn, left = l.split(' ', 1)\n",
    "            \n",
    "            if turn == '1': # new story\n",
    "                story = []\n",
    "\n",
    "            if '\\t' in left: # question\n",
    "                q, a = left.split('\\t', 1)\n",
    "                q = tokenize(q)\n",
    "                q = lower_list(q)\n",
    "                if q[-1] == '?':\n",
    "                    q = q[:-1]\n",
    "                if '\\t' in a:\n",
    "                    a = a.split('\\t')[0] # discard reward\n",
    "                a = a.split('|') # may contain several labels\n",
    "                a = lower_list(a)\n",
    "\n",
    "                substory = [x for x in story if x]\n",
    "\n",
    "                data.append((substory, q, a))\n",
    "                story.append('')\n",
    "            else: # normal sentence\n",
    "                s = tokenize(left)\n",
    "                if s[-1] == '.':\n",
    "                    s = s[:-1]\n",
    "                s = lower_list(s)\n",
    "                story.append(s)\n",
    "\n",
    "    if is_babi:\n",
    "        flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "        data = [(flatten(story), q, answer) for story, q, answer in data if not max_length or len(flatten(story)) < max_length]\n",
    "        \n",
    "    return data\n",
    "\n",
    "def vectorize(data, w2i, story_maxlen, query_maxlen, entities=None):\n",
    "    if entities:\n",
    "        e2i = dict((e, i) for i, e in enumerate(entities))\n",
    "\n",
    "    S, Q, A = [], [], []\n",
    "    for story, question, answer in data:\n",
    "        # Vectroize story\n",
    "        s_pad_len = max(0, story_maxlen - len(story))\n",
    "        s = [w2i[w] for w in story] + [0] * s_pad_len\n",
    "\n",
    "        # Vectroize question\n",
    "        q_pad_len = max(0, query_maxlen - len(question))\n",
    "        q = [w2i[w] for w in question] + [0] * q_pad_len\n",
    "        q = q[:query_maxlen]\n",
    "\n",
    "        # Vectroize answer\n",
    "        if entities:\n",
    "            y = np.zeros(len(entities), dtype='byte')\n",
    "            for a in answer:\n",
    "                y[e2i[a]] = 1\n",
    "        else:\n",
    "            y = np.zeros(len(w2i) + 1) # +1 for nil word\n",
    "            for a in answer:\n",
    "                y[w2i[a]] = 1\n",
    "\n",
    "        S.append(s)\n",
    "        Q.append(q)\n",
    "        A.append(y)\n",
    "    \n",
    "    S = np.array(S, dtype=np.uint16)\n",
    "    Q = np.array(Q, dtype=np.uint16)\n",
    "    A = np.array(A, dtype='byte')\n",
    "\n",
    "    return S, Q, A\n",
    "\n",
    "def vectorize_kv_pairs(kv_pairs, memory_size, entities):\n",
    "    vec_kv_pairs = []\n",
    "    w2i = dict((e, i) for i, e in enumerate(entities))\n",
    "    w2i['directed_by'] = len(w2i)\n",
    "    w2i['written_by'] = len(w2i)\n",
    "    w2i['starred_actors'] = len(w2i)\n",
    "    w2i['release_year'] = len(w2i)\n",
    "    w2i['has_genre'] = len(w2i)\n",
    "    w2i['has_tags'] = len(w2i)\n",
    "    w2i['has_plot'] = len(w2i)\n",
    "    for ent_list in kv_pairs:\n",
    "#         print('----ent_list', ent_list)\n",
    "#         print(len(ent_list))\n",
    "        kv = [w2i[e] for e in ent_list if e in w2i]\n",
    "        mem_pad_len = max(0, memory_size - len(kv))\n",
    "        vec_kv_pairs.append(kv + [0] * mem_pad_len)\n",
    "\n",
    "\n",
    "    return np.array(vec_kv_pairs, dtype=np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_babi = False\n",
    "if is_babi:\n",
    "    train_stories = load_task('./data/tasks_1-20_v1-2/en/qa5_three-arg-relations_train.txt', is_babi)\n",
    "    test_stories = load_task('./data/tasks_1-20_v1-2/en/qa5_three-arg-relations_test.txt', is_babi)\n",
    "else:\n",
    "    N = 50000\n",
    "    train_stories = load_pickle('mov_task1_qa_pipe_train.pickle')[:N]\n",
    "    test_stories = load_pickle('mov_task1_qa_pipe_test.pickle')[:N]\n",
    "    kv_pairs = load_pickle('mov_kv_pairs.pickle')\n",
    "    train_kv_indices = load_pickle('mov_train_kv_indices.pickle')[:N]\n",
    "    test_kv_indices = load_pickle('mov_test_kv_indices.pickle')[:N]\n",
    "    train_kv = [ [kv_pairs[ind] for ind in indices] for indices in train_kv_indices ]\n",
    "    test_kv = [ [kv_pairs[ind] for ind in indices] for indices in test_kv_indices ]\n",
    "    train_kv = np.array([list(chain(*x)) for x in train_kv])\n",
    "    test_kv = np.array([list(chain(*x)) for x in test_kv])\n",
    "    print(len(train_kv), train_kv[0])\n",
    "    \n",
    "    entities = load_pickle('mov_entities.pickle')\n",
    "    entity_size = len(entities)\n",
    "\n",
    "vocab = set()\n",
    "for story, q, answer in train_stories + test_stories:\n",
    "    vocab |= set(story + q + answer)\n",
    "vocab = sorted(vocab)\n",
    "\n",
    "# Reserve 0 for masking via pad_sequences\n",
    "vocab_size = len(vocab) + 1\n",
    "story_maxlen = max(map(len, (x for x, _, _ in train_stories + test_stories)))\n",
    "query_maxlen = max(map(len, (x for _, x, _ in train_stories + test_stories)))\n",
    "\n",
    "print('-')\n",
    "print('Vocab size:', vocab_size, 'unique words')\n",
    "print('Story max length:', story_maxlen, 'words')\n",
    "print('Query max length:', query_maxlen, 'words')\n",
    "print('Number of training stories:', len(train_stories))\n",
    "print('Number of test stories:', len(test_stories))\n",
    "print('-')\n",
    "print('Here\\'s what a \"story\" tuple looks like (input, query, answer):')\n",
    "print(train_stories[0])\n",
    "print('-')\n",
    "print('Vectorizing the word sequences...')\n",
    "\n",
    "print('len(entities)', len(entities))\n",
    "w2i = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "inputs_train, queries_train, answers_train = vectorize(train_stories,\n",
    "                                                               w2i,\n",
    "                                                               story_maxlen,\n",
    "                                                               query_maxlen, entities)\n",
    "inputs_test, queries_test, answers_test = vectorize(test_stories,\n",
    "                                                            w2i,\n",
    "                                                            story_maxlen,\n",
    "                                                            query_maxlen, entities)\n",
    "\n",
    "print('-')\n",
    "print('inputs: integer tensor of shape (samples, max_length)')\n",
    "print('inputs_train shape:', inputs_train.shape)\n",
    "print('inputs_test shape:', inputs_test.shape)\n",
    "print('-')\n",
    "print('queries: integer tensor of shape (samples, max_length)')\n",
    "print('queries_train shape:', queries_train.shape)\n",
    "print('queries_test shape:', queries_test.shape)\n",
    "print('-')\n",
    "print('answers: binary (1 or 0) tensor of shape (samples, vocab_size)')\n",
    "print('answers_train shape:', answers_train.shape)\n",
    "print('answers_test shape:', answers_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train_kv[0]:', train_kv[0], ', mem_size:', len(train_kv[0]))\n",
    "# mem_maxlen = max(map(len, (x for x in train_kv+test_kv)))\n",
    "train_mem_maxlen = max(map(len, (x for x in train_kv)))\n",
    "test_mem_maxlen = max(map(len, (x for x in test_kv)))\n",
    "mem_maxlen = max(train_mem_maxlen, test_mem_maxlen)\n",
    "\n",
    "print('mem_maxlen:', mem_maxlen)\n",
    "vec_train_kv = vectorize_kv_pairs(train_kv, mem_maxlen, vocab)\n",
    "vec_test_kv = vectorize_kv_pairs(test_kv, mem_maxlen, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_kv[0], vec_train_kv[0])\n",
    "print(len(answers_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MemNNKV(mem_size, query_maxlen, vocab_size, entity_size, embd_size):\n",
    "    print('mem_size:', mem_size)\n",
    "    print('q_max', query_maxlen)\n",
    "    print('embd_size', embd_size)\n",
    "    print('vocab_size', vocab_size)\n",
    "    print('entity_size', entity_size)\n",
    "    # placeholders\n",
    "    key = Input((mem_size,), name='Key_Input')\n",
    "    val = Input((mem_size,), name='Val_Input')\n",
    "    question = Input((query_maxlen,), name='Question_Input')\n",
    "\n",
    "    # encoders\n",
    "    # memory encoders\n",
    "    key_encoder = Sequential(name='Key_Encoder')\n",
    "    key_encoder.add(Embedding(input_dim=entity_size, output_dim=embd_size, input_length=mem_size))\n",
    "    val_encoder = Sequential(name='Val_Encoder')\n",
    "    val_encoder.add(Embedding(input_dim=entity_size, output_dim=embd_size, input_length=mem_size))\n",
    "    # output: (samples, mem_size, embd_size)\n",
    "\n",
    "    # embed the question into a sequence of vectors\n",
    "    question_encoder = Sequential(name='Question_Encoder')\n",
    "    question_encoder.add(Embedding(input_dim=vocab_size, output_dim=embd_size, input_length=query_maxlen))\n",
    "#     question_encoder.add(Dropout(0.3))\n",
    "    # output: (samples, query_maxlen, embd_size)\n",
    "\n",
    "    # encode input sequence and questions (which are indices)\n",
    "    # to sequences of dense vectors\n",
    "    key_encoded = key_encoder(key) # (None, mem_size, embd_size)\n",
    "    val_encoded = val_encoder(val) # (None, mem_size, embd_size)\n",
    "    question_encoded = question_encoder(question) # (None, query_max_len, embd_size)\n",
    "\n",
    "    ph = dot([question_encoded, key_encoded], axes=(2, 2)) \n",
    "    ph = Permute((2, 1))(ph) # (None, mem_size, query_max_len)\n",
    "    o = dot([ph, val_encoded], axes=(1, 1)) # (None, query_max_len, embd_size)\n",
    "    R = Dense(embd_size, input_shape=(embd_size,), name='R_Dense')     \n",
    "    q2 = R(add([question_encoded,  o])) # (None, query_max_len, embd_size)\n",
    "    \n",
    "    cand_encoder = Sequential(name='cand_encoder')\n",
    "    cand_encoder.add(Embedding(input_dim=entity_size, output_dim=embd_size, input_length=1))\n",
    "#     cand_encoder.add(Dropout(0.3))\n",
    "    \n",
    "    cand = Input((entity_size,), name='Cand_Input')\n",
    "    y_encoded = cand_encoder(cand) # (None, entity_size, embd_size)\n",
    "#     print('y_encoded', y_encoded.shape)\n",
    "    \n",
    "    answer = dot([q2, y_encoded], axes=(2, 2)) # (None, query_max_len, entity_size)\n",
    "    answer = Lambda(lambda x: K.sum(x, axis=1), output_shape=(entity_size, )) (answer)\n",
    "    preds = Activation('softmax')(answer)\n",
    "    print('--answer', answer.shape)\n",
    "    \n",
    "    # build the final model\n",
    "    model = Model([key, val, question, cand], answer)\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "embd_size = 64\n",
    "memnn_kv = MemNNKV(mem_maxlen, query_maxlen, vocab_size, entity_size, embd_size)\n",
    "print(memnn_kv.summary())\n",
    "# train_cands = \n",
    "memnn_kv.fit([vec_train_kv, vec_train_kv, queries_train, answers_train], answers_train,\n",
    "          batch_size=32,\n",
    "          epochs=10,\n",
    "          validation_data=([vec_test_kv, vec_test_kv, queries_test, answers_test], answers_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MemNN(story_maxlen, query_maxlen, vocab_size, embd_size):\n",
    "    # placeholders\n",
    "    input_sequence = Input((story_maxlen,))\n",
    "    question = Input((query_maxlen,))\n",
    "\n",
    "    # encoders\n",
    "    # embed the input sequence into a sequence of vectors\n",
    "    input_encoder_m = Sequential()\n",
    "    input_encoder_m.add(Embedding(input_dim=vocab_size,\n",
    "                                  output_dim=embd_size))\n",
    "    input_encoder_m.add(Dropout(0.3))\n",
    "    # output: (samples, story_maxlen, embedding_dim)\n",
    "\n",
    "    # embed the input into a sequence of vectors of size query_maxlen\n",
    "    input_encoder_c = Sequential()\n",
    "    input_encoder_c.add(Embedding(input_dim=vocab_size,\n",
    "                                  output_dim=query_maxlen))\n",
    "    input_encoder_c.add(Dropout(0.3))\n",
    "    # output: (samples, story_maxlen, query_maxlen)\n",
    "\n",
    "    # embed the question into a sequence of vectors\n",
    "    question_encoder = Sequential()\n",
    "    question_encoder.add(Embedding(input_dim=vocab_size,\n",
    "                                   output_dim=embd_size,\n",
    "                                   input_length=query_maxlen))\n",
    "    question_encoder.add(Dropout(0.3))\n",
    "    # output: (samples, query_maxlen, embedding_dim)\n",
    "\n",
    "    # encode input sequence and questions (which are indices)\n",
    "    # to sequences of dense vectors\n",
    "    input_encoded_m = input_encoder_m(input_sequence)\n",
    "    input_encoded_c = input_encoder_c(input_sequence)\n",
    "    question_encoded = question_encoder(question) # (None, query_max_len, embd_size)\n",
    "\n",
    "    # compute a 'match' between the first input vector sequence\n",
    "    # and the question vector sequence\n",
    "    # shape: `(samples, story_maxlen, query_maxlen)`\n",
    "    match = dot([input_encoded_m, question_encoded], axes=(2, 2))\n",
    "    match = Activation('softmax')(match) # (None, max_storylen, query_maxlen)\n",
    "\n",
    "    # add the match matrix with the second input vector sequence\n",
    "    response = add([match, input_encoded_c])  # (samples, story_maxlen, query_maxlen)\n",
    "    response = Permute((2, 1))(response)  # (samples, query_maxlen, story_maxlen)\n",
    "    print('---')\n",
    "    print('match',match.shape)\n",
    "    print('input_c', input_encoded_c.shape)\n",
    "    print('response', response.shape)\n",
    "    # concatenate the match matrix with the question vector sequence\n",
    "#     answer = concatenate([response, question_encoded])\n",
    "    answer = response\n",
    "    print('---')\n",
    "    print('resp.shape', response.shape)\n",
    "    print('q_enc.shape', question_encoded.shape)\n",
    "    print('answer.shape', answer.shape)\n",
    "\n",
    "    # the original paper uses a matrix multiplication for this reduction step.\n",
    "    # we choose to use a RNN instead.\n",
    "    answer = LSTM(32)(answer)  # (samples, 32)\n",
    "\n",
    "    # one regularization layer -- more would probably be needed.\n",
    "#     answer = Dropout(0.3)(answer)\n",
    "    answer = Dense(vocab_size)(answer)  # (samples, vocab_size)\n",
    "    # we output a probability distribution over the vocabulary\n",
    "    answer = Activation('softmax')(answer)\n",
    "\n",
    "    # build the final model\n",
    "    model = Model([input_sequence, question], answer)\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "embd_size = 64\n",
    "model = MemNN(story_maxlen, query_maxlen, vocab_size, embd_size)\n",
    "print(model.summary())\n",
    "\n",
    "# train\n",
    "model.fit([inputs_train, queries_train], answers_train,\n",
    "          batch_size=32,\n",
    "          epochs=3,\n",
    "          validation_data=([inputs_test, queries_test], answers_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MemNNKV_bk(mem_size, query_maxlen, vocab_size, embd_size):\n",
    "    print('mem_size:', mem_size)\n",
    "    print('q_max', query_maxlen)\n",
    "    print('embd_size', embd_size)\n",
    "    print('vocab_size', vocab_size)\n",
    "    # placeholders\n",
    "    key = Input((mem_size,))\n",
    "    val = Input((mem_size,))\n",
    "    question = Input((query_maxlen,))\n",
    "\n",
    "    # encoders\n",
    "    # memory encoders\n",
    "    key_encoder = Sequential()\n",
    "    key_encoder.add(Embedding(input_dim=vocab_size, output_dim=embd_size))#, input_length=query_maxlen))\n",
    "    val_encoder = Sequential()\n",
    "    val_encoder.add(Embedding(input_dim=vocab_size, output_dim=query_maxlen))#, input_length=query_maxlen))\n",
    "    # output: (samples, mem_size, embd_size)\n",
    "\n",
    "    # embed the question into a sequence of vectors\n",
    "    question_encoder = Sequential()\n",
    "    question_encoder.add(Embedding(input_dim=vocab_size,\n",
    "                                   output_dim=embd_size,\n",
    "                                   input_length=query_maxlen))\n",
    "    question_encoder.add(Dropout(0.3))\n",
    "    # output: (samples, query_maxlen, embd_size)\n",
    "\n",
    "    # encode input sequence and questions (which are indices)\n",
    "    # to sequences of dense vectors\n",
    "    key_encoded = key_encoder(key)\n",
    "    val_encoded = val_encoder(val)\n",
    "    question_encoded = question_encoder(question) # (None, query_max_len, embd_size)\n",
    "\n",
    "    # compute a 'match' between the first input vector sequence\n",
    "    # and the question vector sequence\n",
    "    # shape: `(samples, mem_size, query_maxlen)`\n",
    "#     match = dot([input_encoded_m, question_encoded], axes=(2, 2))\n",
    "    match = dot([key_encoded, question_encoded], axes=(2, 2))\n",
    "    match = Activation('softmax')(match) # (None, mem_size, query_maxlen)\n",
    "    print('match.shape', match.shape)\n",
    "\n",
    "    # add the match matrix with the second input vector sequence\n",
    "#     response = add([match, input_encoded_c])  # (samples, mem_size, query_maxlen)\n",
    "    response = add([match, val_encoded])  # (samples, mem_size, query_maxlen)\n",
    "    response = Permute((2, 1))(response)  # (samples, query_maxlen, mem_size)\n",
    "\n",
    "    # concatenate the match matrix with the question vector sequence\n",
    "    answer = concatenate([response, question_encoded])\n",
    "    print('---')\n",
    "    print('resp.shape', response.shape)\n",
    "    print('q_enc.shape', question_encoded.shape)\n",
    "    print('answer.shape', answer.shape)\n",
    "\n",
    "    # the original paper uses a matrix multiplication for this reduction step.\n",
    "    # we choose to use a RNN instead.\n",
    "    answer = LSTM(32)(answer)  # (samples, 32)\n",
    "\n",
    "    # one regularization layer -- more would probably be needed.\n",
    "    answer = Dropout(0.3)(answer)\n",
    "    answer = Dense(vocab_size)(answer)  # (samples, vocab_size)\n",
    "    # we output a probability distribution over the vocabulary\n",
    "    answer = Activation('softmax')(answer)\n",
    "\n",
    "    # build the final model\n",
    "    model = Model([key, val, question], answer)\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# embd_size = 64\n",
    "# memnn_kv = MemNNKV(story_maxlen, query_maxlen, vocab_size, embd_size)\n",
    "# print('---input_train', inputs_train.shape)\n",
    "# memnn_kv.fit([inputs_train, inputs_train, queries_train], answers_train,\n",
    "#           batch_size=32,\n",
    "#           epochs=10,\n",
    "#           validation_data=([inputs_train, inputs_train, queries_test], answers_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
