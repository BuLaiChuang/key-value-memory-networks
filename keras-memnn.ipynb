{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Activation, Dense, Lambda, Permute, Dropout, add, dot, concatenate, multiply\n",
    "from keras.layers import LSTM\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from functools import reduce\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.callbacks import ModelCheckpoi`nt\n",
    "import datetime\n",
    "from itertools import chain\n",
    "from process_data import load_entities, save_pickle, load_pickle, load_kv_pairs, lower_list, vectorize, vectorize_kv, find_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_babi = False\n",
    "if is_babi:\n",
    "    train_data = load_task('./data/tasks_1-20_v1-2/en/qa5_three-arg-relations_train.txt', is_babi)\n",
    "    test_data = load_task('./data/tasks_1-20_v1-2/en/qa5_three-arg-relations_test.txt', is_babi)\n",
    "else:\n",
    "    # N = 49900\n",
    "    N = 500000000\n",
    "    mem_maxlen = 100 # 1つのエピソードに関連しているKVの数に対する制限\n",
    "    train_data = load_pickle('mov_task1_qa_pipe_train.pickle')[:N]\n",
    "    test_data = load_pickle('mov_task1_qa_pipe_test.pickle')[:N]\n",
    "    kv_pairs = load_pickle('mov_kv_pairs.pickle')\n",
    "    train_k = load_pickle('mov_train_k.pickle')\n",
    "    train_v = load_pickle('mov_train_v.pickle')\n",
    "    test_k = load_pickle('mov_test_k.pickle')\n",
    "    test_v = load_pickle('mov_test_v.pickle')\n",
    "#     train_k = np.array([list(chain(*x))[:mem_maxlen] for x in train_k])\n",
    "#     train_v = np.array([list(chain(*x))[:mem_maxlen] for x in train_v])\n",
    "#     test_k = np.array([list(chain(*x))[:mem_maxlen] for x in test_k])\n",
    "#     test_v = np.array([list(chain(*x))[:mem_maxlen] for x in test_v])\n",
    "    train_k = np.array(train_k)\n",
    "    train_v = np.array(train_v)\n",
    "    test_k = np.array(test_k)\n",
    "    test_v = np.array(test_v)\n",
    "    entities = load_pickle('mov_entities.pickle')\n",
    "    entity_size = len(entities)\n",
    "\n",
    "vocab = set(entities + ['directed_by', 'written_by', 'starred_actors', 'release_year', 'has_genre', 'has_tags', 'has_plot'] \n",
    "                     + ['!directed_by', '!written_by', '!starred_actors', '!release_year', '!has_genre', '!has_tags', '!has_plot'] )\n",
    "for _, q, answer in train_data + test_data:\n",
    "    vocab |= set(q + answer)\n",
    "vocab = sorted(vocab)\n",
    "save_pickle(vocab, 'mov_vocab.pickle')\n",
    "# vocab = load_pickle('mov_vocab.pickle')\n",
    "stopwords = load_pickle('mov_stopwords.pickle')\n",
    "\n",
    "w2i = dict((c, i) for i, c in enumerate(vocab))\n",
    "i2w = dict((i, c) for i, c in enumerate(vocab))\n",
    "save_pickle(w2i, 'mov_w2i.pickle')\n",
    "save_pickle(i2w, 'mov_i2w.pickle')\n",
    "# w2i = load_pickle('mov_w2i.pickle')\n",
    "# i2w = load_pickle('mov_i2w.pickle')\n",
    "\n",
    "# Reserve 0 for masking via pad_sequences\n",
    "vocab_size = len(vocab) #+ 1\n",
    "# story_maxlen = max(map(len, (x for x, _, _ in train_data + test_data)))\n",
    "query_maxlen = max(map(len, (x for _, x, _ in train_data + test_data)))\n",
    "\n",
    "print('-')\n",
    "print('Vocab size:', vocab_size, 'unique words')\n",
    "# print('Story max length:', story_maxlen, 'words')\n",
    "print('Query max length:', query_maxlen, 'words')\n",
    "print('Number of training data:', len(train_data))\n",
    "print('Number of test data:', len(test_data))\n",
    "print('-')\n",
    "print('Here\\'s what a \"data\" tuple looks like (input, query, answer):')\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_k_max_memsize = max(map(len, (d for d in train_k)))\n",
    "train_v_max_memsize = max(map(len, (d for d in train_v)))\n",
    "test_k_max_memsize = max(map(len, (d for d in test_k)))\n",
    "test_v_max_memsize = max(map(len, (d for d in test_v)))\n",
    "# print(train_k_max_memsize, train_v_max_memsize)\n",
    "# for i, (k,v) in enumerate(zip(train_k, train_v)):\n",
    "#     print(len(k), len(v))\n",
    "#     if i > 100: break\n",
    "#     continue\n",
    "#     if len(k) == 12082:\n",
    "#         print(i)\n",
    "#         pp.pprint(k[:199])\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(find_ngrams(entities, 'the movie a boy and his dog, when was it released'.split(' '), 100))\n",
    "print(find_ngrams(entities, tokenize('the movie a boy and his dog, when was it released'), 100))\n",
    "print(find_ngrams(entities, tokenize('w.s. van dyke was the director on which movies?'), 100))\n",
    "print(find_ngrams(entities, word_tokenize('w.s. van dyke was the director on which movies?'), 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def find_ngrams(token_dict, text, n):\n",
    "#     # base case\n",
    "#     if n <= 1:\n",
    "#         return text\n",
    "#     # tokens committed to output\n",
    "#     saved_tokens = []\n",
    "#     # tokens remaining to be searched in sentence\n",
    "#     search_tokens = text[:]\n",
    "#     # tokens stored until next ngram found\n",
    "#     next_search = []\n",
    "#     while len(search_tokens) >= n:\n",
    "#         ngram = ' '.join(search_tokens[:n])\n",
    "#         if ngram in token_dict:\n",
    "#             # first, search previous unmatched words for smaller ngrams\n",
    "#             sub_n = min(len(next_search), n - 1)\n",
    "#             saved_tokens.extend(find_ngrams(token_dict, next_search, sub_n))\n",
    "#             next_search.clear()\n",
    "#             # then add this ngram\n",
    "#             saved_tokens.append(ngram)\n",
    "#             # then pop this ngram from the remaining words to search\n",
    "#             search_tokens = search_tokens[n:]\n",
    "#         else:\n",
    "#             next_search.append(search_tokens.pop(0))\n",
    "#     remainder = next_search + search_tokens\n",
    "#     sub_n = min(len(remainder), n - 1)\n",
    "#     saved_tokens.extend(find_ngrams(token_dict, remainder, sub_n))\n",
    "#     return saved_tokens\n",
    "# t = 'what films are about w.s. van dyke'\n",
    "# # t = ['what', 'films', 'are', 'about', 'w.s', '.', 'van', 'dyke'] \n",
    "# # t = ['what', 'films', 'are', 'about', 'steven', 'brill']\n",
    "# find_ngrams(entities, t.split(' '), 1000)\n",
    "\n",
    "# def tokenize(sent):\n",
    "#     '''Return the tokens of a sentence including punctuation.\n",
    "#     >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
    "#     ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
    "#     '''\n",
    "#     return [x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]\n",
    "# tokenize('what films are about w.s. van dyke')\n",
    "# tokenize('when was the movie Sleep, My Love released?')\n",
    "# ts = 'when was the movie sleep, my love released?'.split(' ')\n",
    "# print(find_ngrams(entities, ts, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# kv_pairs[1000:1050]\n",
    "# # kv_pairs = load_pickle('mov_kv_pairs.pickle')\n",
    "# # print(kv_pairs[1])\n",
    "print(train_k[0])\n",
    "print(train_v[0])\n",
    "print(vec_train_k[0])\n",
    "# stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2i['what']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = []\n",
    "for _,_,a in (train_data+test_data):\n",
    "    if a[0] not in label_list: label_list.append(a[0])\n",
    "    \n",
    "#     for aa in a:\n",
    "#         if aa not in label_list: label_list.append(aa)\n",
    "print(len(label_list), label_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2i_label = dict((c, i) for i, c in enumerate(label_list))\n",
    "i2w_label = dict((i, c) for i, c in enumerate(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def vectorize(data, w2i, story_maxlen, query_maxlen, w2i_label):\n",
    "    Q, A = [], []\n",
    "    for story, question, answer in data:\n",
    "        # Vectroize question\n",
    "        q = [w2i[w] for w in question if w in w2i]\n",
    "        q = q[:query_maxlen]\n",
    "        q_pad_len = max(0, query_maxlen - len(q))\n",
    "        q += [0] * q_pad_len\n",
    "\n",
    "#         y = np.zeros(len(w2i_label))\n",
    "#         y[w2i_label[answer[0]]] = 1\n",
    "        y = np.zeros(len(w2i))\n",
    "        y[w2i[answer[0]]] = 1\n",
    "#         for a in answer:\n",
    "#             y[w2i[a]] = 1\n",
    "\n",
    "        Q.append(q)\n",
    "        A.append(y)\n",
    "    \n",
    "    Q = np.array(Q, dtype=np.uint32)\n",
    "    A = np.array(A, dtype='byte')\n",
    "\n",
    "    return Q, A\n",
    "\n",
    "queries_train, answers_train = vectorize(train_data,\n",
    "                                           w2i,\n",
    "                                           0,\n",
    "                                           query_maxlen, w2i_label)\n",
    "queries_test, answers_test = vectorize(test_data,\n",
    "                                            w2i,\n",
    "                                            0,\n",
    "                                            query_maxlen, w2i_label)\n",
    "\n",
    "print('-')\n",
    "print('inputs: integer tensor of shape (samples, max_length)')\n",
    "# print('inputs_train shape:', inputs_train.shape)\n",
    "# print('inputs_test shape:', inputs_test.shape)\n",
    "print('-')\n",
    "print('queries: integer tensor of shape (samples, max_length)')\n",
    "print('queries_train shape:', queries_train.shape)\n",
    "print('queries_test shape:', queries_test.shape)\n",
    "print('-')\n",
    "print('answers: binary (1 or 0) tensor of shape (samples, vocab_size)')\n",
    "print('answers_test shape:', answers_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chk_id = 100\n",
    "print(train_data[chk_id][1])\n",
    "print([w2i[w] for w in train_data[chk_id][1] if w in w2i])\n",
    "# print([i2w[w] for w in train_data[0][1]])\n",
    "print(queries_train[chk_id])\n",
    "print([ i2w[i] for i in queries_train[chk_id]])\n",
    "pp.pprint(train_k[chk_id])\n",
    "pp.pprint(train_v[chk_id])\n",
    "print(vec_train_k[chk_id]) -\n",
    "print(answers_train[chk_id], i2w_label[np.argmax(answers_train[chk_id])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_memory_num = 100\n",
    "# vec_train_k = vectorize_kv(train_k, mem_maxlen, w2i)\n",
    "# vec_train_v = vectorize_kv(train_v, mem_maxlen, w2i)\n",
    "# vec_test_k = vectorize_kv(test_k, mem_maxlen, w2i)\n",
    "# vec_test_v = vectorize_kv(test_v, mem_maxlen, w2i)\n",
    "# print('vec_k', vec_train_k.shape)\n",
    "# print('vec_v', vec_train_v.shape)\n",
    "def vectorize_kv(data, max_mem_len, max_mem_size, w2i):\n",
    "    all_vec_list = []\n",
    "    for i, kv_list in enumerate(data):\n",
    "        if i % 5000 == 0: print(i, '/', len(data))\n",
    "        vec_list = []\n",
    "        for kv in kv_list:\n",
    "            vec = [w2i[e] for e in kv if e in w2i]\n",
    "            vec = vec[:max_mem_len]\n",
    "            mem_pad_len = max(0, max_mem_len - len(vec))\n",
    "            vec = vec + [0] * mem_pad_len\n",
    "            vec_list.append(vec)\n",
    "        vec_list = vec_list[:max_mem_size]\n",
    "        mem_pad_size = max(0, max_mem_size - len(vec_list))\n",
    "        for _ in range(mem_pad_size):\n",
    "            vec_list.append([0] * max_mem_len)\n",
    "        all_vec_list.append(vec_list)\n",
    "\n",
    "    return np.array(all_vec_list, dtype=np.uint32)\n",
    "\n",
    "max_mem_len = 4\n",
    "max_mem_size = 15\n",
    "vec_train_k = vectorize_kv(train_k, max_mem_len, max_mem_size, w2i)\n",
    "vec_train_v = vectorize_kv(train_v, max_mem_len, max_mem_size, w2i)\n",
    "vec_test_k = vectorize_kv(test_k, max_mem_len, max_mem_size, w2i)\n",
    "vec_test_v = vectorize_kv(test_v, max_mem_len, max_mem_size, w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MemNNKV(mem_len, mem_size, query_maxlen, vocab_size, embd_size, answer_size):\n",
    "    print('mem_size:', mem_size)\n",
    "    print('q_max', query_maxlen)\n",
    "    print('embd_size', embd_size)\n",
    "    print('vocab_size', vocab_size)\n",
    "#     print('entity_size', entity_size)\n",
    "    print('-----------')\n",
    "\n",
    "    # placeholders\n",
    "    key = Input((mem_size, mem_len,), name='Key_Input')\n",
    "    val = Input((mem_size, mem_len,), name='Val_Input')\n",
    "    question = Input((query_maxlen,), name='Question_Input')\n",
    "    print('key:', key.shape)\n",
    "\n",
    "    # encoders\n",
    "    # memory encoders\n",
    "    # output: (None, mem_size, embd_size)\n",
    "    shared_embd_A = Embedding(input_dim=vocab_size, output_dim=embd_size)\n",
    "\n",
    "    # embed the question into a sequence of vectors\n",
    "    # output: (None, query_maxlen, embd_size)\n",
    "    question_encoder = Sequential(name='Question_Encoder')\n",
    "    question_encoder.add(shared_embd_A)\n",
    "#     question_encoder.add(Dropout(0.3))\n",
    "\n",
    "    # encode input sequence and questions (which are indices)\n",
    "    # to sequences of dense vectors\n",
    "    key_encoded = shared_embd_A(key) # (None, mem_size, mem_len, embd_size)\n",
    "    print('key_encoded', key_encoded.shape)\n",
    "    key_encoded = Lambda(lambda x: K.sum(x, axis=2)) (key_encoded) #(None, mem_size, embd_size)\n",
    "    print('key_encoded', key_encoded.shape)\n",
    "    val_encoded = shared_embd_A(val) # (None, mem_size, embd_size)\n",
    "    val_encoded = Lambda(lambda x: K.sum(x, axis=2)) (val_encoded)\n",
    "    \n",
    "    question_encoded = question_encoder(question) # (None, query_max_len, embd_size)\n",
    "#     question_encoded = Lambda(lambda x: K.sum(x, axis=1)) (val_encoded)\n",
    "    question_encoded = Lambda(lambda x: K.sum(x, axis=1)) (question_encoded) #(None, embd_size)\n",
    "    print('q_encoded', question_encoded.shape)\n",
    "    q= question_encoded\n",
    "    for h in range(1):\n",
    "        print('---hop', h)\n",
    "        ph = dot([q, key_encoded], axes=(1, 2))  # (None, mem_size)\n",
    "        print('ph', ph.shape)\n",
    "        ph = Activation('softmax')(ph)\n",
    "        o = multiply([ph, Permute((2, 1))(val_encoded)]) # (None, embd_size, mem_size)\n",
    "        print('o', o.shape)\n",
    "        o = Lambda(lambda x: K.sum(x, axis=2))(o) # (None, embd_size)\n",
    "        print('o', o.shape)\n",
    "        R = Dense(embd_size, input_shape=(embd_size,), name='R_Dense_h' + str(h+1))\n",
    "        q = R(add([q,  o])) # (None, embd_size)\n",
    "        print('q', q.shape)\n",
    "\n",
    "#     answer = Dense(answer_size, name='last_Dense')(q) #(None, answer_size)\n",
    "    answer = Dense(vocab_size, name='last_Dense')(q) #(None, vocab_size)\n",
    "    print('answer.shape', answer.shape)\n",
    "    preds = Activation('softmax')(answer)\n",
    "    \n",
    "    # build the final model\n",
    "    model = Model([key, val, question], preds)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "    return model\n",
    "\n",
    "print('vec_test_k.shape', vec_train_k.shape)\n",
    "print('vec_test_v.shape', vec_train_v.shape)\n",
    "print('queries_train.shape', queries_train.shape)\n",
    "print('ans', answers_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embd_size = 200\n",
    "memnn_kv = MemNNKV(max_mem_len, max_mem_size, query_maxlen, vocab_size, embd_size, len(label_list))\n",
    "print(memnn_kv.summary())\n",
    "now = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "model_path = 'saved_models/' + now + '_kvnn-weights-{epoch:02d}-{loss:.4f}.hdf5'\n",
    "checkpoint = ModelCheckpoint(model_path, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "memnn_kv.fit([vec_train_k, vec_train_v, queries_train], answers_train,\n",
    "          batch_size=64,\n",
    "          epochs=30,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_data=([vec_test_k, vec_test_v, queries_test], answers_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "memnn_kv.save('model_memnn_kv.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model('demo_model_memnn_kv.h5') # heavy to run\n",
    "# score = model.evaluate([vec_test_k, vec_test_v, queries_test], answers_test, verbose=1)\n",
    "# score = model.evaluate([vec_train_k, vec_train_v, queries_train], answers_train, verbose=1)\n",
    "# print('Test loss:', score[0])\n",
    "# print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pred = model.predict([vec_train_k, vec_train_v, queries_train], verbose=1) \n",
    "pred = load_pickle('pred.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, (p, a) in enumerate(zip(pred, answers_train[:len(pred)])):\n",
    "    pw = i2w[np.argmax(p)]\n",
    "    pa = i2w[np.argmax(a)]\n",
    "    if pw == pa:\n",
    "        print(i, ' '.join(train_data[i][1]))\n",
    "        print(pw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('q.shape', queries_test.shape)\n",
    "ret_predict = model.predict([vec_test_k, vec_test_v, queries_test], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('q:',test_data[0][1])\n",
    "print('predict:',i2w[np.argmax(ret_predict[0])])\n",
    "print('label:',i2w[np.argmax(answers_test[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stopwords = load_pickle('mov_stopwords.pickle')\n",
    "\n",
    "# tokenize a question\n",
    "q = 'who directed blade runner'\n",
    "q_tokens = word_tokenize(q)\n",
    "q_tokens = find_ngrams(vocab, q_tokens, 100000)\n",
    "print('q_tokens:', q_tokens)\n",
    "\n",
    "# vectorize a question\n",
    "q_pad_len = max(0, query_maxlen - len(q_tokens))\n",
    "vec_q = [w2i[w] for w in q_tokens] + [0] * q_pad_len\n",
    "vec_q = np.array(vec_q)\n",
    "vec_q = np.reshape(vec_q, (1, len(vec_q)))\n",
    "print('vec_q:', vec_q)\n",
    "\n",
    "# get related kv\n",
    "k_list, v_list = [], []\n",
    "for w in q_tokens:\n",
    "    if w not in stopwords:\n",
    "        for kv_ind, (k, v) in enumerate(kv_pairs):\n",
    "            if w in (k+v):\n",
    "                k_list += k\n",
    "                v_list += v\n",
    "    else:\n",
    "        print(w, 'in stopwords')\n",
    "\n",
    "def _vec_kv(data, w2i, mem_maxlen):\n",
    "    vec = [w2i[e] for e in data if e in w2i]\n",
    "    vec += [0] * max(0, mem_maxlen - len(vec))\n",
    "    vec = vec[:mem_maxlen]\n",
    "    vec = np.array(vec)\n",
    "#     vec = np.expand_dims(vec, axis=0)\n",
    "    print('---', vec.shape)\n",
    "    vec = np.reshape(vec, (1, 100))\n",
    "    \n",
    "    return vec\n",
    "# print(k_list)\n",
    "# vectroize kv\n",
    "vec_k, vec_v = None, None\n",
    "vec_k = _vec_kv(k_list, w2i, mem_maxlen)\n",
    "vec_v = _vec_kv(v_list, w2i, mem_maxlen)\n",
    "# vec_k = [w2i[e] for e in k_list if e in w2i]\n",
    "# vec_k += [0] * max(0, mem_maxlen - len(vec_k))\n",
    "# vec_k = vec_k[:mem_maxlen]\n",
    "# vec_k = np.array(vec_k)\n",
    "# vec_v = [w2i[e] for e in v_list if e in w2i]\n",
    "# vec_v += [0] * max(0, mem_maxlen - len(vec_v))\n",
    "# vec_v = vec_v[:mem_maxlen]\n",
    "# vec_v = np.array(vec_v)\n",
    "print(vec_k.shape)\n",
    "\n",
    "int_predict = model.predict([vec_k, vec_v, vec_q], batch_size=1, verbose=1)     \n",
    "print('q:',q)\n",
    "print('predict:',i2w[np.argmax(int_predict[0])])\n",
    "# print('label:',i2w[np.argmax(answers_test[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for story, q, answer in (train_data + test_data):#[:100]:\n",
    "#     print(story + q + answer)\n",
    "    vocab |= set(story + q + answer)\n",
    "vocab = sorted(vocab)\n",
    "print(len(list(set(vocab))))\n",
    "print(vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MemNN(story_maxlen, query_maxlen, vocab_size, embd_size):\n",
    "    # placeholders\n",
    "    input_sequence = Input((story_maxlen,))\n",
    "    question = Input((query_maxlen,))\n",
    "\n",
    "    # encoders\n",
    "    # embed the input sequence into a sequence of vectors\n",
    "    input_encoder_m = Sequential()\n",
    "    input_encoder_m.add(Embedding(input_dim=vocab_size,\n",
    "                                  output_dim=embd_size))\n",
    "    input_encoder_m.add(Dropout(0.3))\n",
    "    # output: (samples, story_maxlen, embedding_dim)\n",
    "\n",
    "    # embed the input into a sequence of vectors of size query_maxlen\n",
    "    input_encoder_c = Sequential()\n",
    "    input_encoder_c.add(Embedding(input_dim=vocab_size,\n",
    "                                  output_dim=query_maxlen))\n",
    "    input_encoder_c.add(Dropout(0.3))\n",
    "    # output: (samples, story_maxlen, query_maxlen)\n",
    "\n",
    "    # embed the question into a sequence of vectors\n",
    "    question_encoder = Sequential()\n",
    "    question_encoder.add(Embedding(input_dim=vocab_size,\n",
    "                                   output_dim=embd_size,\n",
    "                                   input_length=query_maxlen))\n",
    "    question_encoder.add(Dropout(0.3))\n",
    "    # output: (samples, query_maxlen, embedding_dim)\n",
    "\n",
    "    # encode input sequence and questions (which are indices)\n",
    "    # to sequences of dense vectors\n",
    "    input_encoded_m = input_encoder_m(input_sequence)\n",
    "    input_encoded_c = input_encoder_c(input_sequence)\n",
    "    question_encoded = question_encoder(question) # (None, query_max_len, embd_size)\n",
    "\n",
    "    # compute a 'match' between the first input vector sequence\n",
    "    # and the question vector sequence\n",
    "    # shape: `(samples, story_maxlen, query_maxlen)`\n",
    "    match = dot([input_encoded_m, question_encoded], axes=(2, 2))\n",
    "    match = Activation('softmax')(match) # (None, max_storylen, query_maxlen)\n",
    "\n",
    "    # add the match matrix with the second input vector sequence\n",
    "    response = add([match, input_encoded_c])  # (samples, story_maxlen, query_maxlen)\n",
    "    response = Permute((2, 1))(response)  # (samples, query_maxlen, story_maxlen)\n",
    "    print('---')\n",
    "    print('match',match.shape)\n",
    "    print('input_c', input_encoded_c.shape)\n",
    "    print('response', response.shape)\n",
    "    # concatenate the match matrix with the question vector sequence\n",
    "#     answer = concatenate([response, question_encoded])\n",
    "    answer = response\n",
    "    print('---')\n",
    "    print('resp.shape', response.shape)\n",
    "    print('q_enc.shape', question_encoded.shape)\n",
    "    print('answer.shape', answer.shape)\n",
    "\n",
    "    # the original paper uses a matrix multiplication for this reduction step.\n",
    "    # we choose to use a RNN instead.\n",
    "    answer = LSTM(32)(answer)  # (samples, 32)\n",
    "\n",
    "    # one regularization layer -- more would probably be needed.\n",
    "#     answer = Dropout(0.3)(answer)\n",
    "    answer = Dense(vocab_size)(answer)  # (samples, vocab_size)\n",
    "    # we output a probability distribution over the vocabulary\n",
    "    answer = Activation('softmax')(answer)\n",
    "\n",
    "    # build the final model\n",
    "    model = Model([input_sequence, question], answer)\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "embd_size = 64\n",
    "model = MemNN(story_maxlen, query_maxlen, vocab_size, embd_size)\n",
    "print(model.summary())\n",
    "\n",
    "# train\n",
    "model_path = 'kvnn-weights-{ecposh:02d}-{loss:.4f}.hdf5'\n",
    "checkpoint = ModelCheckpoint(model_path, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "model.fit([inputs_train, queries_train], answers_train,\n",
    "          batch_size=32,\n",
    "          epochs=3,\n",
    "          callbacks=callbacks_list\n",
    "          validation_data=([inputs_test, queries_test], answers_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(1000, 64, input_length=10))\n",
    "# the model will take as input an integer matrix of size (batch, input_length).\n",
    "# the largest integer (i.e. word index) in the input should be no larger than 999 (vocabulary size).\n",
    "# now model.output_shape == (None, 10, 64), where None is the batch dimension.\n",
    "\n",
    "input_array = np.random.randint(1000, size=(32, 10))\n",
    "\n",
    "model.compile('rmsprop', 'mse')\n",
    "print(model.shape)\n",
    "output_array = model.predict(input_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
